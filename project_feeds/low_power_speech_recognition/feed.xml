<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Low Power Speech Recognition</title>
    <link href="https://dakpro.github.io/project_feeds/low_power_speech_recognition/"/>
    <id>urn:uuid:60a76c80-d399-11d9-b93C-0003939e0af6</id>
    <updated>2025-07-30T10:59:00Z</updated>
    <author>
        <name>Danylo Kvit</name>
        <email>kvit.2006@gmail.com</email>
    </author>
    
    <entry>
        <title>Week 3</title>
        <link href="https://dakpro.github.io/project_feeds/low_power_speech_recognition/week3"/>
        <id>urn:uuid:062a1210-a952-48be-9d8d-f02c5c276682</id>
        <updated>2025-07-30T10:59:00Z</updated>
        <content type="html"><![CDATA[<h2 id="week-3">Week 3</h2>
<p>(Note: this blog will be updated throughout the week)</p>
<p><code>nvidia/parakeet-tdt_ctc-110m</code> - cannot be run on rPi:
when trying to run the program, it just exits after a while at the point of importing the
<code>nemo.collections.asr</code>.</p>
<p>As discovered later on, all nvidia models require nvidia GPU to run. Thus we are left with
<code>moonhsine</code>.</p>
<p>Also came across <code>vosk</code> and <code>faster-whisper</code> which are interesting to try.</p>
<h3 id="results-and-comparison">Results and Comparison:</h3>
<h4 id="moonshine-tiny">Moonshine tiny</h4>
<p>And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.</p>
<h4 id="moonshinebase">Moonshine/base</h4>
<p>And so my fellow Americans ask not what your country can do for you ask what you can do for your country</p>
<table>
    <tr>
        <th> Model</th>
        <th> 11s transcription time </th>
        <th> Word Error Rate </th>
    </tr>
    <tr>
      <td> whisper.cpp/base </td>
      <td> 21 s </td>
      <td> 10.32 </td>
    </tr>
    <tr>
      <td> whisper.cpp/base-Q4_K </td>
      <td> 12.6 s </td>
      <td> -- </td>
    </tr>
    <tr>
      <td> Moonshine/base </td>
      <td> 2.76 s </td>
      <td> 9.99 </td>
    </tr>
    <tr>
      <td> whisper.cpp/tiny </td>
      <td> 8.3 s </td>
      <td> 12.81 </td>
    </tr>
    <tr>
      <td> Moonshine/tiny </td>
      <td> 1.48 s </td>
      <td> 12.65 </td>
    </tr>
</table>

<h3 id="connecting-microphone-to-rpi">Connecting microphone to rPi</h3>
<p>Just connect it via USB.
Run <code>arecord -l</code> to see information about connected audio devices, say card X and device Y.</p>
<p>To make it a default audio input device (strongly recommended), add this into ~/.asoundrc:</p>
<pre><code>
pcm.!default{
    type hw
    card X
}

ctl.!default{
    type hw
    card X
}
</code></pre>

<p>You can test it with</p>
<pre><code>
# record
arecord -D plughw:X,Y -f cd -t wav -d 5 test.wav
# play
aplay test.wav
</code></pre>

<h3 id="moonshine-in-streaming-mode">Moonshine in streaming mode</h3>
<p>Simple demo:</p>
<pre><code>
git clone https://github.com/moonshine-ai/moonshine
uv pip install numba
uv pip install -r moonshine/demo/moonshine-onnx/requirements.txt
sudo apt update
sudo apt upgrade -y
sudo apt install -y portaudio19-dev
# run:
python3 moonshine/demo/moonshine-onnx/live_captions.py
</code></pre>




<h3 id="testing-on-realisticly-long-audios">Testing on realisticly long audios</h3>
<caption>Datasets used for the <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">model leaderboard</a></caption>
![Models](week3.png)

<p>From the listed above, I chose SPGISpeech, Earnings-22, and AMI for evalutaion of a model, as the model will be mostly used during meetings.</p>
<p>The raw datasets are can be included</p>
]]></content>
    </entry>

    <entry>
        <title>Week 2_2</title>
        <link href="https://dakpro.github.io/project_feeds/low_power_speech_recognition/week2_2"/>
        <id>urn:uuid:5ce0118b-ad98-441d-b041-896a4287b46c</id>
        <updated>2025-07-29T10:37:00Z</updated>
        <content type="html"><![CDATA[<h2 id="week-2-part-2">Week 2 part 2</h2>
<p>After evaluation of previous models&#39; performances, we decided to try to fit Voxtral - another transformer model.</p>
<p>The mini version of the model is 3b parameters, weights 8 gb, which took quite a long time to download even for Mac. As kyutai with 1b parameters was way too slow on rPi, I decided that there&#39;s no point in trying to run Voxtral on rPi.</p>
<p>At this point it became obvious that most models are made for powerful devices with GPU. Thus, a decision was made to rather look for a model definitely smaller than 1b params rather than trying out every model we pass by.</p>
<p>Of course the exact speed of the model depends on the pipeline itself but the constant
factor caused by this cannot outweight the fact that kuytai took about 10s to transcribe 1s
of audio on 4/4 threads.</p>
<h3 id="hugging-face">Hugging face</h3>
<p>Hugging face is an open-source platform for AI models. Similar to github, not only it provides most (if not all) models with their &quot;model cards&quot;, but also has leaderboards for the models. This is what I&#39;ll be working with next.</p>
<p><a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">Here</a> one can find
the leaderboard of the speech-recognition models. We are interested in two criteria: WER (word error rate) and RTFx (time of the audio being transcribed/transcription time).</p>
<p>The tiny.en model without quantization has RTFx of 348, base.en has 320.</p>
<p>Interesting model:</p>
<p>UsefulSensors/moonshine-tiny - 9.99 / 565.97</p>
<p>The following seem extremely fast too, but later turned out that they require Nvidia GPU architecture</p>
<p>nvidia/parakeet-tdt_ctc-110m - 7.49 /
5345.14</p>
<p>nvidia/parakeet-tdt-0.6b-v2  - 6.05
3386.02</p>
<p>nvidia/canary-180m-flash     - 7.12 / 1213.58</p>
<p>nvidia/parakeet-rnnt-0.6b    - 7.5  / 2815.72      (no punctuation/capitalization)</p>
<p>nvidia/parakeet-ctc-0.6b     - 7.69 / 4281.53       (no punctuation/capitalization)</p>
]]></content>
    </entry>

    <entry>
        <title>Week 2_1</title>
        <link href="https://dakpro.github.io/project_feeds/low_power_speech_recognition/week2_1"/>
        <id>urn:uuid:26a31438-e93b-469e-97df-f5543150a1f6</id>
        <updated>2025-07-23T13:50:00Z</updated>
        <content type="html"><![CDATA[<h2 id="week-2-part-1">Week 2 part 1</h2>
<p>From last week, problem of memory shortage exists: track of memory usage shows that the process tries to use more and more memory, resulting in a crash and thus the process being killed by the OS.</p>
<p>Solution 1:
Using microSD partially as RAM:</p>
<pre><code>
# Enabling usage of 8GB for swapping
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Making it permanent
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

...

# Disabling swapping
sudo swapoff /swapfile

# Permanent disabling
sudo rm /swapfile
... (remove line from fstab)
sudo reboot
</code></pre>

<p>This showed that the model needs only 1.6GB more memory. As microSD memory is too slow, the model running took enormous time to complete and thus was terminated.</p>
<p>One could 1) use ssd instead - too costly and crosses idea of small-power; 2) use rPi with bigger RAM (currenty 4 gb).</p>
<h2 id="returning-back-to-whispercpp">Returning back to whisper.cpp</h2>
<h3 id="evaluation-of-the-three-models">Evaluation of the three models</h3>
<p>Decided to do evaluation of speed of transcription using different models.</p>
<p>Here is time and memory usage for transcribing an 11s JFK speech using 4/4 threads and standart OS:</p>
<table>
    <tr>
        <th>Model</th>
        <th>Time</th>
        <th>Memory</th>
    </tr>
    <tr>
        <td>tiny</td>
        <td>8.3 s</td>
        <td>77 MB</td>
    </tr>
    <tr>
        <td>tiny.en</td>
        <td>8.5 s</td>
        <td>77 MB</td>
    </tr>
    <tr>
        <td>base</td>
        <td>18 s</td>
        <td>147 MB</td>
    </tr>
    <tr>
        <td>base.en</td>
        <td>21 s</td>
        <td>256MB</td>
    </tr>
    <tr>
        <td>small</td>
        <td>64 s</td>
        <td>487 MB</td>
    </tr>
    <tr>
        <td>small.en</td>
        <td>65 s</td>
        <td>487 MB</td>
    </tr>
</table>

<p>The performance test was performed once and only on one recording.</p>
<p>Optimization of loading time and other inter-sample could be considered for real-time transcription.</p>
<p><i>Same evaluation on rPi 5 (possibly with 8gb RAM) could be reasonable due to CPU difference, but despite being 2x times faster, it requires fan/active cooling.</i></p>
<p>After iterational refinement, the following script is used as <code>~/eval.sh</code> for evaluation:</p>
<pre><code>
#!/bin/bash

models=()
while [ $# -gt 0 ]; do
  models+=( "$1" )
  shift
done

echo "models: ${models[@]}"
touch report.log
echo "Report on model evaluation. The duration of sample recording is 11s (JFK speech)" > report.log
cd whisper.cpp
echo -n "Building whisper-cli... "
cmake -B build > /dev/null
cmake --build build -j --config Release > /dev/null
echo "whisper-cli build"
base_models=("tiny" "tiny.en" "base" "base.en" "small" "small.en" "medium" "medium.en")
echo "-----------------------------"
echo "-----------------------------" >> ../report.log

is_base_model(){
     for bm in "${base_models[@]}"; do
    if [[ "$1" =~ ^"${bm}"$ ]]; then
       echo "$1 IS base model"
       return 0
    fi
  done
    echo "$1 is not a base model"
    return 1
}


for model in "${models[@]}"; do
    echo "Model $model" >> ../report.log
    if is_base_model $model; then
        echo "Starting model $model evaluation"
        if [ ! -f models/$model.bin ]; then
            echo -n "Model not found... Downloading $model... "
                  sh ./models/download-ggml-model.sh $model > /dev/null
            mv models/ggml-$model.bin models/$model.bin
                  echo "Downloaded"
        fi
        path="models/$model.bin"
    else
        echo -n "Looking for quantized model $model... "
        if [ ! -f quantized_models/$model.bin ]; then
            echo "Quantized model not found. Skipping..."
            continue
        fi
        path="quantized_models/$model.bin"
        echo "Quantized model found"
    fi
    echo -n "Runtime: " >> ../report.log
    echo -n "Running $model... "
    ./build/bin/whisper-cli -m $path -f samples/jfk.wav > tmp.out 2>&1

  # for debugging
  # cat tmp.out

  grep -i -E "total memory|total time" tmp.out >> ../report.log
  echo "run"
  echo "----------------------------------" >> ../report.log
  echo "----------------------------------"
done
</code></pre>

<h3 id="quantization-of-whisper">Quantization of whisper</h3>
<p>Unlike kyutai, whisper supports built-in quantization.</p>
<p>Notes on choosing quantizations:</p>
<p>Qx_y - x bits per weight, y - legacy flag, deprecated in favour of Qx_K</p>
<p>Qx_K - K-quants, better than standard, have mixed bit-widths</p>
<p>TQx - ternary quantization (ters instead of bits), extreme compression and quality drops too much</p>
<p>IQx_s - importance-aware quantization, much better quality for the same bit rates. s - size (S/M/L)</p>
<p>Based on this, will try with IQ4_M first.</p>
<p>After iterational refinement, this script was used as <code>~/qt.sh</code> for quantization:</p>
<pre><code>

#!/bin/bash

echo "args: $@"

cd whisper.cpp
if [ $# -eq 0 ]; then
    echo "Error: quantization method is not provided."
    echo "Usage: $0 <quantization method 1> ... [-r <model: default:base>] "
    exit 1
fi
qms=()
model="base"
while [ $# -gt 0 ]; do
    echo "curr arg: $1"
    if [[ "$1" == "-m" ]]; then
        echo "equals to -m"
        shift
        model="$1"
        break
    fi
    qms+=("$1")
    shift
done
echo "qms: ${sqm[@]}"

if [ ! -d "quantized_models" ]; then
    mkdir quantized_models
fi
for qm in "${qms[@]}"; do
    ./build/bin/quantize models/$model.bin quantized_models/$model-$qm.bin $qm
done

</code></pre>

<hr>
<p>After spending some time figuring why the model doesn&#39;t want to be quantized to IQ4_M, it turns out that models possible for quantization are listed in lines 50-80 of file common-ggml.cpp.</p>
<p>After small experimenting with <code>base</code> model:</p>
<p>q5_0 - improvement from 18.1 to 14.3 (encoding time: 14.5 to 11.4 )</p>
<p>q2_k - model starts outputing &quot;you you you&quot; -&gt; not enough quality</p>
<p>q5_k - improvement from 18.1 to 13.2 (encoding time: 14.7 to 10.6)</p>
<p>Further evaluations:</p>
<h3 id="model-evaluation-on-11s-sample">Model Evaluation on 11s sample</h3>
<caption> Model Evaluation Report (11s JFK Speech Sample)</caption>
<table>
    <thead>
      <tr>
        <th>Model</th>
        <th>Runtime (s)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td colspan="2" style="background-color: #e0e0e0; font-weight: bold;">Small Models</td>
      </tr>
      <tr>
        <td>small-q2_k</td>
        <td>38.4</td>
      </tr>
      <tr>
        <td>small-q3_k</td>
        <td>46.2</td>
      </tr>
      <tr>
        <td>small-q4_0</td>
        <td>39.8</td>
      </tr>
      <tr>
        <td>small-q4_1</td>
        <td>39.1</td>
      </tr>
      <tr>
        <td>small-q4_k</td>
        <td>37.3</td>
      </tr>
      <tr>
        <td>small-q5_0</td>
        <td>47</td>
      </tr>
      <tr>
        <td>small-q5_1</td>
        <td>49.7</td>
      </tr>
      <tr>
        <td>small-q5_k</td>
        <td>44.7</td>
      </tr>
      <tr>
        <td>small-q6_k</td>
        <td>46.6</td>
      </tr>
      <tr>
        <td>small-q8_0</td>
        <td>40.5</td>
      </tr>
      <tr>
        <td>small</td>
        <td>76.3</td>
      </tr>
      <tr>
        <td colspan="2" style="background-color: #e0e0e0; font-weight: bold;">Base Models</td>
      </tr>
      <tr>
        <td>base-q2_k</td>
        <td>75.9</td>
      </tr>
      <tr>
        <td>base-q3_k</td>
        <td>13.7</td>
      </tr>
      <tr>
        <td>base-q4_0</td>
        <td>12.6</td>
      </tr>
      <tr>
        <td>base-q4_1</td>
        <td>12.3</td>
      </tr>
      <tr>
        <td>base-q4_k</td>
        <td>11.9</td>
      </tr>
      <tr>
        <td>base-q5_0</td>
        <td>14.4</td>
      </tr>
      <tr>
        <td>base-q5_1</td>
        <td>14.4</td>
      </tr>
      <tr>
        <td>base-q5_k</td>
        <td>13.3</td>
      </tr>
      <tr>
        <td>base-q6_k</td>
        <td>13.6</td>
      </tr>
      <tr>
        <td>base-q8_0</td>
        <td>12.8</td>
      </tr>
      <tr>
        <td>base</td>
        <td>18.2</td>
      </tr>
    </tbody>
  </table>

<p>Issue: q2_k should be smaller and faster, while it&#39;s not. Small-q2_k doesn&#39;t get stuck and actually produces the correct transcription, so performance decrease is somewhere else.</p>
<p>Turns out q2_k/q3_k are optimized for AVX2/AVX512 (Single Instruction, Multiple Data commands extensions) in x86 architecture. For rPi running on ARM CPU, those are absent and quantization overhead becomes cosmic, thus slowing down in performance. Model getting stuck on &quot;you you you&quot; is likely result of poor resulting precision of the model.</p>
<p>In theory, base-q4_k run on a headless setup should be sufficient at least for with additional bit of time for transcription (for instance, additional 5-10 mins after an hour-long meeting). But if we want to achieve real-time
transcription, one should seek for alternatives.</p>
]]></content>
    </entry>

    <entry>
        <title>Week 1</title>
        <link href="https://dakpro.github.io/project_feeds/low_power_speech_recognition/week1"/>
        <id>urn:uuid:1225c695-cfb8-4ebb-aaaa-80da344efa6a</id>
        <updated>2025-07-18T14:39:00Z</updated>
        <content type="html"><![CDATA[<h2 id="whisper">Whisper</h2>
<p>Went through the paper on Whisper - speech recognition model from OpenAI.</p>
<p>It&#39;s open source and available on GitHub.</p>
<p>Many models are available to choose from:</p>
<p><img src="week1.1.png" alt="Models">
Choice of model:</p>
<ol>
<li>By taking into account other processes running on the device -- better for deployment</li>
<li>Customizable by user?</li>
</ol>

<p><i>There can be some custom vocabulary/promting added to the model -- interesting what it can be achieved with it.</i></p>
<p>Training dataset is 2/3 english and 1/3 uneven mix, but model&#39;s &quot;knowledge&quot; is transferable across the languages (for instance slavic languages parts enhance each other).</p>
<p>Installed both whisper and whisper.cpp on Mac</p>
<p>Ran transcription with whisper</p>
<p>Ran transcription with whisper.cpp</p>
<p><code>sox -d &lt;filename&gt;</code>
nice tool to record audio
-d stands for default input device</p>
<h2 id="rpi">rPi</h2>
<p>Tried to set up the rPI. The system didn&#39;t boot. Turns out it&#39;s the problem with the rPi itself - it didn&#39;t read from the SD card (indication of no reading: no green LED blinking, only red).</p>
<p>Got new board - gives green light</p>
<h2 id="new-rpi">new rPi</h2>
<p>Booting rPi with 64-bit standart (not headless) OS.
<i>for production and further testing - headless (Lite) version should be tested as it&#39;s smaller and faster than the standart OS.</i></p>
<h3 id="connecting-mac-to-the-rpi-ssh-via-ethernet-via-switch">Connecting Mac to the rPi ssh via ethernet via switch</h3>
<p>! don&#39;t forget about setting host when writing OS to the SD-card</p>
<p><i>just figured out you can update bootloader with the same sd - just different stuff needs to be loaded on it. Could I fix the &quot;broken&quot; rPi by updating the boot? (to be done)</i></p>
<ol>
<li>connect both rPi and Mac to an ethernet switch (NetGear GS108 in my case)</li>

<p><i>Had problem with detecting connection from rPi to the switch.</i></p>
<li>When using ethernet on Mac, one should add the ethernet as service. (Done in *Settings/Network*)</li>

<li>To make the connection work, one should make static IP addresses on the connection for both Mac and rPi</li>
</ol>

<p>For Mac:</p>
<ol>
<li>goto Settings/Network/Apple Adapter(or how else you named the service) -> Details -> TCP/IP tab</li>
<li>change configure ipv4 to manual</li>
<li>Input the static address (I chose 192.168.5.1)</li>
<li>Subnet mask is left 255.255.0.0, other empty fields are left empty</li>
</ol>

<p>For standart rPi setup:</p>
<ol>
<li>Click on the double-arrow network symbol in the top right corner</li>
<li>Advanced Options/Edit Connections/Wired Connection X/IPv4 Settings/</li>
<i>note: previously set Link negotiation on Wired Connection X/Ethernet to Automatic - what has it fixed??</i>
<i>also set cloned MAC address to Permanent - not sure I completely understand what it does</i>
<li>Set *Method* to *Manual*</li>
<li>*Add*</li>
<li>Set parameters (192.168.5.2, 24, 192.168.5.1 for me (not sure what 24 does))</li>
<li>Save</li>
<li>Reboot the rPi</li>
</ol>

<p>For headless rPi setup:<strong>TODO</strong></p>
<p>Finally, we got the working rPi-Mac connection</p>
<p>To verify: turn off wifi and try
<code>ping raspberrypi.local</code>
Or even try to login (on my rPi I made user = &quot;user&quot;):
<code>ssh <a href="mailto:&#117;&#x73;&#x65;&#x72;&#x40;&#x72;&#97;&#115;&#112;&#x62;&#101;&#x72;&#x72;&#x79;&#112;&#105;&#x2e;&#x6c;&#x6f;&#x63;&#x61;&#x6c;">&#117;&#x73;&#x65;&#x72;&#x40;&#x72;&#97;&#115;&#112;&#x62;&#101;&#x72;&#x72;&#x79;&#112;&#105;&#x2e;&#x6c;&#x6f;&#x63;&#x61;&#x6c;</a></code>
Also ensure in .ssh/known_hosts there&#39;s no entry for raspberrypi.local, as there exists a  with such URL, thus when you try to connect to ssh for the first time the website is accessed.</p>
<h3 id="connecting-rpi-to-eduroam-via-wlan">Connecting rPi to eduroam via wlan</h3>
<p>needs to be done via loading configuration as /etc/wpa_supplicant/wpa_supplicant.conf:</p>
<pre><code>
network={
  ssid="eduroam"
  key_mgmt=WPA-EAP
  eap=PEAP
  identity="<token name>"
  password="<password>"
  phase1="peaplabel=0"
  phase2="auth=MSCHAPV2"
  ca_cert="<pathToCertificate>"
  priority=1
}
</code></pre>

<p>restarting the service:</p>
<pre><code>
sudo killall wpa_supplicant
sudo wpa_supplicant -B -i wlan0 -c /etc/wpa_supplicant/wpa_supplicant.conf
sudo dhclient wlan0
</code></pre>

<p>check by</p>
<pre><code>
iwgetid
ping 1.1.1.1
</code></pre>

<h3 id="ran-whispercpp-on-rpi">Ran whisper.cpp on rPi</h3>
<p>Took ~18s to transcribe 11s audio.
Lite OS optimization wouldn&#39;t be that effective + other processes are to be run in the background.</p>
<p>Before thinking on optimization decided to run kyutai, as if kyutai is 5 times faster, optimization efforts are wasted.</p>
<h2 id="kyutai">Kyutai</h2>
<p>Alternative model: kyutai</p>
<ul>
<li>Smaller, better performance than whisper</li>
<li>Inputs stream instead of recording, thus much better for live transcription</li>
<li>Only English and French</li>
</ul>
<p>Trying to run kyutai model on rPi</p>
<ol>
<li>Clone repo from git</li>
<li>Install rust</li>
<li>cd stt-rs</li>
<li>sudo apt install libssl-dev</li>
<li>export PKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig</li>
<li>cargo run -r ../audio/bria.mp3</li>
</ol>
<i>takes a long to build - haven't tried with <code>uv</code> though</i>

<p><i>github guide also includes &quot;--features cuda&quot; in the last stage, but as there&#39;s no gpu on rPi, it&#39;s been removed</i></p>
<p>Problem: kyutai is too big and thus cannot fit into 3.3 RAM -&gt; the process gets killed</p>
<p>sudo install python-msgpack</p>
]]></content>
    </entry>
</feed>